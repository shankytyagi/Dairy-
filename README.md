Cyber security Expert :
https://en.m.wikipedia.org/wiki/Martin_Hellman
AI Expert
https://en.m.wikipedia.org/wiki/Andrew_Ng

The transformer is a deep learning architecture that was developed by researchers at Google and is based on the multi-head attention mechanism, which was proposed in the 2017 paper "Attention Is All You Need".

Text is converted to numerical representations called tokens, and each token is converted into a vector via lookup from a word embedding table.[1] At each layer, each token is then contextualized within the scope of the context window with other (unmasked) tokens via a parallel multi-head attention mechanism, allowing the signal for key tokens to be amplified and less important tokens to be diminished.
Source : 
https://en.m.wikipedia.org/wiki/Transformer_(deep_learning_architecture)

Contact : Shankyedtech23@gmail.com
Till 18/4/25
1) AI, ML, Analytics Training. 
2) IELTS, Duolingo, English  Training. 
3) Maths Coaching

ich bin shanky tyagi. 
je suis shanky tyagi
Soy Shanky Tyagi

Guess the meaning of word of year 2024 : Brain rot? ( Next Day Dairy)
